{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018/2019 - Task List 10\n",
    "\n",
    "1. Implement Naive Bayes classifier with pyro\n",
    "    - create apropriate parameters (mean and std for a and b, sigma - noise)\n",
    "    - provide optimization procedure\n",
    "    - check appropriateness of implemented method with selected dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.optim as optim\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TracePredictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(\n",
    "    iris.data, iris.target, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB accuracy: 0.960902 (0.035958)\n",
      "NB f1_micro: 0.960902 (0.035958)\n",
      "NB f1_macro: 0.960191 (0.036241)\n"
     ]
    }
   ],
   "source": [
    "kfold = 5\n",
    "scoring = ['accuracy', 'f1_micro', 'f1_macro']\n",
    "cv_results = model_selection.cross_validate(gnb, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "#displaying the mean and standard deviation of the prediction\n",
    "for score in scoring: \n",
    "    msg = \"%s: %f (%f)\" % ('NB ' + score, cv_results['test_' + score].mean(), cv_results['test_' + score].std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self-made NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = x_data\n",
    "        self.y = y_data\n",
    "        self.attributes_number = x_data.shape[1]\n",
    "        self.classes = np.unique(y_data)\n",
    "    \n",
    "    def fit(self):\n",
    "        div_data = self.div_by_category_and_attribute(self.x, self.y)\n",
    "        self.params = {}\n",
    "        for key in div_data.keys():\n",
    "            mean, scale, losses = self.train(div_data[key], key)\n",
    "            self.params[key] = {\"mean\":mean, \"scale\":scale}\n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        pass\n",
    "    \n",
    "    def div_by_category_and_attribute(self, x_data, y_data):\n",
    "        X_cl = {}\n",
    "\n",
    "        for i in range(len(x_data)):\n",
    "            for attribute in range(len(x_data[0])):\n",
    "                if (y_data[i], attribute) not in X_cl.keys():\n",
    "                    X_cl[(y_data[i], attribute)] = list()\n",
    "                X_cl[(y_data[i], attribute)].append(x_data[i][attribute])\n",
    "\n",
    "        for cl_key in np.unique(y_data):\n",
    "            for attribute in range(len(x_data[0])):\n",
    "                X_cl[cl_key, attribute] = np.array(X_cl[cl_key, attribute])\n",
    "\n",
    "        return X_cl\n",
    "    \n",
    "    def model(self, x_data, label):\n",
    "        mean = pyro.param('mean' + str(label),  torch.tensor(np.random.choice(x_data, 1)).double())\n",
    "        scale = pyro.param('scale' + str(label), torch.tensor(1.0).double(), constraint=constraints.positive)\n",
    "        with pyro.plate('data_loop' + str(label), len(x_data)):\n",
    "            pyro.sample('prob' + str(label), dist.Normal(mean, scale), obs=x_data)\n",
    "\n",
    "    def guide(self, x_data, label):\n",
    "        mean = pyro.param('mean' + str(label), torch.tensor(2.0))\n",
    "        scale = pyro.param('scale' + str(label), torch.tensor(1.0), constraint=constraints.positive)\n",
    "        pyro.sample('prob' + str(label), dist.Normal(mean.double(), scale.double()), infer={'is_auxiliary': True})\n",
    "        \n",
    "    def train(self, data, label, num_steps=1000):\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        optim = pyro.optim.Adam({\"lr\": 0.045})\n",
    "        svi = pyro.infer.SVI(model=self.model,\n",
    "                             guide=self.guide,\n",
    "                             optim=optim,\n",
    "                             loss=pyro.infer.Trace_ELBO(),\n",
    "                             num_samples=len(data))\n",
    "\n",
    "        losses = []\n",
    "        print(\"Learning for class: \" + str(label[0]) + \" and attribute: \" + str(label[1]))\n",
    "        t = tqdm(range(num_steps))\n",
    "        for j in t:\n",
    "            loss = svi.step(torch.from_numpy(data), label)\n",
    "            losses.append(loss)\n",
    "            t.set_postfix(loss=loss)\n",
    "        return pyro.param(\"mean\" + str(label)), pyro.param(\"scale\" + str(label)), losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc = NaiveBayesClassifier(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1000 [00:00<00:06, 163.40it/s, loss=89.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 2 and attribute: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 190.43it/s, loss=31.5]\n",
      "  2%|▏         | 17/1000 [00:00<00:05, 168.00it/s, loss=6.87]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 2 and attribute: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 204.76it/s, loss=7.41]\n",
      "  2%|▏         | 22/1000 [00:00<00:04, 211.42it/s, loss=73.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 2 and attribute: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 208.05it/s, loss=25.1]\n",
      "  2%|▏         | 21/1000 [00:00<00:04, 205.82it/s, loss=2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 2 and attribute: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 198.93it/s, loss=1.58] \n",
      "  2%|▏         | 20/1000 [00:00<00:05, 193.42it/s, loss=69.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 0 and attribute: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 206.66it/s, loss=16.4]\n",
      "  2%|▏         | 19/1000 [00:00<00:05, 183.87it/s, loss=22.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 0 and attribute: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 207.91it/s, loss=18.7]\n",
      "  2%|▏         | 19/1000 [00:00<00:05, 189.82it/s, loss=-7.98]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 0 and attribute: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 198.25it/s, loss=-7.86]\n",
      "  2%|▏         | 21/1000 [00:00<00:04, 200.34it/s, loss=32.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 0 and attribute: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 204.96it/s, loss=-34] \n",
      "  2%|▏         | 21/1000 [00:00<00:04, 207.64it/s, loss=76.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning for class: 1 and attribute: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 568/1000 [00:03<00:02, 201.42it/s, loss=25]  "
     ]
    }
   ],
   "source": [
    "nbc.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_by_category_and_attribute(x_data, y_data):\n",
    "    X_cl = {}\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        for attribute in range(len(x_data[0])):\n",
    "            if (y_data[i], attribute) not in X_cl.keys():\n",
    "                X_cl[(y_data[i], attribute)] = list()\n",
    "            X_cl[(y_data[i], attribute)].append(x_data[i][attribute])\n",
    "\n",
    "    for cl_key in np.unique(y_data):\n",
    "        for attribute in range(len(x_data[0])):\n",
    "            X_cl[cl_key, attribute] = np.array(X_cl[cl_key, attribute])\n",
    "\n",
    "    return X_cl\n",
    "\n",
    "def div_by_attribute(x_data, y_data):\n",
    "    X_attr = {}\n",
    "\n",
    "    for i in range(len(x_data)):\n",
    "        for attribute in range(len(x_data[0])):\n",
    "            if attribute not in X_attr.keys():\n",
    "                X_attr[attribute] = list()\n",
    "            X_attr[attribute].append(x_data[i][attribute])\n",
    "\n",
    "    for attribute in range(len(x_data[0])):\n",
    "        X_attr[attribute] = np.array(X_attr[attribute])\n",
    "\n",
    "    return X_attr\n",
    "\n",
    "def model(x_data, label):\n",
    "    #mean = torch.tensor(np.random.choice(x_data, 1)).double()\n",
    "    mean = pyro.param('mean' + str(label),  torch.tensor(np.random.choice(x_data, 1)).double())\n",
    "    scale = pyro.param('scale' + str(label), torch.tensor(1.0).double(), constraint=constraints.positive)\n",
    "    with pyro.plate('data_loop' + str(label), len(x_data)):\n",
    "        pyro.sample('prob' + str(label), dist.Normal(mean, scale), obs=x_data)\n",
    "\n",
    "def guide(x_data, label):\n",
    "    mean = pyro.param('mean' + str(label), torch.tensor(2.0))\n",
    "    scale = pyro.param('scale' + str(label), torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample('prob' + str(label), dist.Normal(mean.double(), scale.double()), infer={'is_auxiliary': True})\n",
    "    \n",
    "# def model_caterogical(data, attribute):\n",
    "#     alpha = torch.tensor(6.0)\n",
    "#     beta = torch.tensor(10.0)\n",
    "#     pay_probs = pyro.sample('class_probs', dist.Beta(alpha, beta).expand(3).independent(1))\n",
    "#     normalized_class_probs = class_probs / torch.sum(class_probs)\n",
    "\n",
    "#     with pyro.iarange('data_loop', len(data)):\n",
    "#         pyro.sample('obs', dist.Categorical(probs=normalized_class_probs), obs=data)\n",
    "\n",
    "# def guide_categorical(x_data, label):\n",
    "#     def guide(data):\n",
    "#         alphas = pyro.param('alphas', torch.tensor(6.).expand(3), constraint=constraints.positive)\n",
    "#         betas = pyro.param('betas', torch.tensor(10.).expand(3), constraint=constraints.positive) \n",
    "#         pyro.sample('class_probs', dist.Beta(alphas, betas).independent(1))\n",
    "\n",
    "def train(data, label, num_steps=3000):\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    optim = pyro.optim.Adam({\"lr\": 0.045})\n",
    "    svi = pyro.infer.SVI(model=model,\n",
    "                         guide=guide,\n",
    "                         optim=optim,\n",
    "                         loss=pyro.infer.Trace_ELBO(),\n",
    "                         num_samples=len(data))\n",
    "\n",
    "    losses = []\n",
    "    t = tqdm(range(num_steps))\n",
    "    for j in t:\n",
    "        loss = svi.step(torch.from_numpy(data), label)\n",
    "        losses.append(loss)\n",
    "        t.set_postfix(loss=loss)\n",
    "    return pyro.param(\"mean\" + str(label)), pyro.param(\"scale\" + str(label)), losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "divided_data = div_by_category_and_attribute(X_train, Y_train)\n",
    "mean, scale, losses = train(divided_data[(0, 0)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    print(pyro.sample('test', dist.Normal(torch.zeros(1, 1), 1.).independent(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    print(pyro.sample('test', dist.Normal(torch.zeros(1, 1), 1.).to_event(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
